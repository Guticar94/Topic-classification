{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#Librerias para parsear documentos\n",
    "from bs4 import BeautifulSoup\n",
    "from os import listdir\n",
    "import re\n",
    "\n",
    "#LibrerÃ­as de procesamiento\n",
    "from pyspark.sql import SparkSession\n",
    "import databricks.koalas as ks\n",
    "from pyspark.sql.types import StructType,StructField, StringType"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.0\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName('lectura')\n",
    "    .getOrCreate())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/07/19 15:10:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "\n",
    "documentos = listdir('/Users/andresfelipegutierrezcarreno/Downloads/2020/')\n",
    "len(documentos)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13287"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "abstracts = []\n",
    "path = '/Users/andresfelipegutierrezcarreno/Downloads/2020/'\n",
    "for doc in listdir(path):\n",
    "  try:\n",
    "    with open(path+doc, 'r') as f:\n",
    "      data = f.read()\n",
    "    Bs_data = BeautifulSoup(data, \"html\")\n",
    "    abstracts.append('.'.join([field.text for field in Bs_data.find_all(re.compile('^awardtitle$|^abstractnarration$'))]))\n",
    "\n",
    "  except:\n",
    "    print('Error en ', doc)\n",
    "    pass\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/andresfelipegutierrezcarreno/Documents/Otros/Topic-classification/venv/lib/python3.8/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error en  ~$035358.xml\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "len(abstracts)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13286"
      ]
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "model_df = spark.sparkContext.parallelize(abstracts).map(lambda x: (x, )).toDF(['Abstract'])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/07/19 18:01:25 WARN TaskSetManager: Stage 14 contains a task of very large size (2484 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#Escritura de abstracs en Parquet para exportar\n",
    "model_df.write.parquet(\"abstracts/df.parquet\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/07/19 18:04:02 WARN TaskSetManager: Stage 16 contains a task of very large size (2484 KiB). The maximum recommended task size is 1000 KiB.\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 67,58% for 10 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 61,43% for 11 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 56,31% for 12 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 51,98% for 13 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 48,27% for 14 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 45,05% for 15 writers\n",
      "22/07/19 18:04:02 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 42,24% for 16 writers\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 45,05% for 15 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 48,27% for 14 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 51,98% for 13 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 56,31% for 12 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 61,43% for 11 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 67,58% for 10 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 75,08% for 9 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 84,47% for 8 writers\n",
      "22/07/19 18:04:04 WARN MemoryManager: Total allocation exceeds 95,00% (906.992.014 bytes) of heap memory\n",
      "Scaling row group sizes to 96,54% for 7 writers\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "22/07/20 02:50:21 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 7205449 ms exceeds timeout 120000 ms\n",
      "22/07/20 02:50:21 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "6bb62b726a54cedd2e458fd073fccc7d621fb74708914dc0dff890e4726c78b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}